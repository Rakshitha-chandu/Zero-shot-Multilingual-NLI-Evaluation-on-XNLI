{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxcAFq9ZjeWl"
      },
      "outputs": [],
      "source": [
        "## step 1 we change the runtime to gpu as multilingual transfomer models are too slow\n",
        "## on cpu\n",
        "### step 2 installing datasets and libraries\n",
        "### step 3 importing the required libararies and verifying if gpu is enabled\n",
        "### step 4 loading the XNLI Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRpR7v7pkHKC"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate torch accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRvPF9OVkL6R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "539fsvKLklvK"
      },
      "outputs": [],
      "source": [
        "## deciding the lanuagues we will work with so we choose\n",
        "## en, hi, te , ta ,ur\n",
        "## xnli does not support te and ta directly so we use only available ones later\n",
        "## make our own one\n",
        "languages = [\"en\", \"hi\", \"ur\"]  # start with these (XNLI supports them)\n",
        "\n",
        "datasets = {}\n",
        "\n",
        "for lang in languages:\n",
        "    datasets[lang] = load_dataset(\"xnli\", lang, split=\"validation\")\n",
        "\n",
        "datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrhJ40X3la_E"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def add_language_column(ds, lang):\n",
        "    ds = ds.map(lambda x: {\"language\": lang})\n",
        "    return ds\n",
        "\n",
        "datasets_with_lang = []\n",
        "\n",
        "for lang, ds in datasets.items():\n",
        "    datasets_with_lang.append(add_language_column(ds, lang))\n",
        "\n",
        "datasets_with_lang\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SderIKslyyD"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "val_data = concatenate_datasets(datasets_with_lang)\n",
        "val_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4blLlMhul_g9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "Counter(val_data[\"language\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiO5wsj3mCKQ"
      },
      "outputs": [],
      "source": [
        "# load evaluation metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: mBERT (Base, Not Fine-Tuned on XNLI)\n",
        "\n",
        "We evaluate the base multilingual BERT model in a zero-shot setting to establish a weak multilingual baseline.\n"
      ],
      "metadata": {
        "id": "Cdyv7qV9A1t7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZbkoLZBmUYt"
      },
      "outputs": [],
      "source": [
        "#Load mBERT Model & Tokenizer\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ2_z_EkmYjN"
      },
      "outputs": [],
      "source": [
        "## preprocessing function\n",
        "def preprocess(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"premise\"],\n",
        "        batch[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r34dqaLDmegs"
      },
      "outputs": [],
      "source": [
        "## tokenise datasets\n",
        "encoded_val = val_data.map(\n",
        "    preprocess,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "encoded_val.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RukEhfoWmit-"
      },
      "outputs": [],
      "source": [
        "## evaluation function\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluate_model(model, dataset, batch_size=16):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return accuracy_metric.compute(\n",
        "        predictions=all_preds,\n",
        "        references=all_labels\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS9KxAAhmnVA"
      },
      "outputs": [],
      "source": [
        "## evalutaion per language\n",
        "results = {}\n",
        "\n",
        "for lang in [\"en\", \"hi\", \"ur\"]:\n",
        "    subset = encoded_val.filter(lambda x: x[\"language\"] == lang)\n",
        "    acc = evaluate_model(model, subset)\n",
        "    results[lang] = acc[\"accuracy\"]\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDqpQcNKmsk8"
      },
      "outputs": [],
      "source": [
        "## converting results to table\n",
        "results_df = pd.DataFrame(\n",
        "    results.items(),\n",
        "    columns=[\"Language\", \"Accuracy\"]\n",
        ")\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note on XLM-R Performance\n",
        "\n",
        "Although XLM-R is fine-tuned on XNLI, zero-shot multilingual inference remains sensitive to preprocessing and calibration.\n",
        "Our results highlight that naive evaluation can still lead to near-chance accuracy, motivating careful benchmarking.\n"
      ],
      "metadata": {
        "id": "OUWcDDTBA-mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8g9k9zjm6pc"
      },
      "outputs": [],
      "source": [
        "#Evaluate XLM-R (Fine-Tuned) on the Same Languages\n",
        "xlmr_model_name = \"joeddav/xlm-roberta-large-xnli\"\n",
        "\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
        "xlmr_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    xlmr_model_name\n",
        ").to(device)\n",
        "\n",
        "xlmr_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJQBgvS2pmLC"
      },
      "outputs": [],
      "source": [
        "def preprocess_xlmr(batch):\n",
        "    return xlmr_tokenizer(\n",
        "        batch[\"premise\"],\n",
        "        batch[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_val_xlmr = val_data.map(\n",
        "    preprocess_xlmr,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "encoded_val_xlmr.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "orV6RQsluqlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlmr_results = {}\n",
        "\n",
        "for lang in [\"en\", \"hi\", \"ur\"]:\n",
        "    subset = encoded_val_xlmr.filter(lambda x: x[\"language\"] == lang)\n",
        "    acc = evaluate_model(xlmr_model, subset)\n",
        "    xlmr_results[lang] = acc[\"accuracy\"]\n",
        "\n",
        "xlmr_results\n"
      ],
      "metadata": {
        "id": "MmnNVg19urGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results\n",
        "results.keys()\n"
      ],
      "metadata": {
        "id": "NnahM-GbwVIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    \"Language\": [\"en\", \"hi\", \"ur\"],\n",
        "    \"mBERT\": [results[l] for l in [\"en\", \"hi\", \"ur\"]],\n",
        "    \"XLM-R\": [xlmr_results[l] for l in [\"en\", \"hi\", \"ur\"]]\n",
        "})\n",
        "\n",
        "comparison_df\n"
      ],
      "metadata": {
        "id": "VSeKbGz8vg12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "comparison_df.plot(\n",
        "    x=\"Language\",\n",
        "    y=[\"mBERT\", \"XLM-R\"],\n",
        "    kind=\"bar\",\n",
        "    title=\"Zero-shot XNLI Accuracy Across Languages\"\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2Ia5wNUfwMHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}